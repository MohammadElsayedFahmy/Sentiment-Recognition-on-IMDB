{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>>> # MLND Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">>> # Sentiment Classification with IMDB Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Classification is a common task in the field of Natural Language Processing, it is about extracting the sentiment of a person given a text they wrote as input.\n",
    "\n",
    "Extracting sentiment from text can lead to build more intelligent systems acting according to those sentiments like chatbots or smart assistants.\n",
    "\n",
    "Although this task is considered a complex task even for humans due to absence of facial expressions and voice tones in text but it is fairly much easier when given a paragraph of more of text written for the purpose of delivering sentiment such as the case with movie reviews.\n",
    "\n",
    "The dataset was constructed back in 2011 by Andrew L. Mass et al. in the paper “Learning Word Vectors for Sentiment Analysis”, was a Kaggle challenge in 2016 and considered a Sentiment Analysis benchmark.\n",
    "Another Dataset used for this task is \"Rotten Tomatoes\" Dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the 50k labeled IMDB movie reviews in text format it is required to extract the sentiment and classify each review (from the 25k test set) to be either positive or negative, then compare the predicted results to the real labels and output the classification accuracy.\n",
    "\n",
    "The problem is to be solved by extracting features out of blocks of text (reviews) and mapping feature values and combinations to either positive or negative sentiment classes through the usage of a couple of Supervised Machine Learning Algorithms (Logistic regression and Naive Bayes), Also some text preprocessing will be done (lemmatization,stop-words removal,punctuation removal ..etc) to facilitate and improve quality of extracted features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric used to evaluate the model is the Accuracy because it is a classification task and all we care about is whether or not the model can extract the sentiment from the reviews correctly.\n",
    "\n",
    "So we determine how well the model is performing by finding the percentage of the correctly predicted labels.\n",
    "\n",
    "Accuracy= (number of correct labels) / (total number of labels) * 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMDB movie review dataset was first proposed by Maas et al.in ”Learning Word Vectors for Sentiment Analysis” as a benchmark for sentiment analysis.\n",
    "\n",
    "The dataset consists of 100K IMDB movie reviews and each review has several sentences.\n",
    "The 100K reviews are divided into three datasets: 25K labeled training instances, 25K\n",
    "labeled test instances and 50K unlabeled training instances. Each review consists of several sentences of text in a file and has one label representing its sentiment: Positive or Negative. These labels are balanced in both\n",
    "the training and the test set.\n",
    "\n",
    "The dataset can be obtained from : http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "I will use the 50k labeled section of the dataset to train my supervised learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell should be uncommented at the first run to make sure all needed libraries for the project and packages are installed (assuming user has Jupyter Notebooks installed and all default ML libraries like numpy and sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install -U stop-words\n",
    "# !python3 -m pip install -U nltk\n",
    "# !python3 -m pip install -U gensim\n",
    "# !python3 -m pip install -U bs4\n",
    "# !python3 -m pip install -U tqdm\n",
    "#!python3 -m pip install -U seaborn\n",
    "# import nltk\n",
    "# nltk.download('popular')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural Language Toolkit : a famous python lib for text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "#BeautifulSoup used for text cleaning (removing HTML tags here)\n",
    "from bs4 import BeautifulSoup\n",
    "#Glob for dealing with files and directories\n",
    "import glob\n",
    "from functools import reduce\n",
    "import operator\n",
    "#tqdm for progress bars\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zmAm1ddciU2q"
   },
   "source": [
    "#### Downloading dataset,extracting compressed files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell should be uncommented on first run to download dataset to the directory and uncompress it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1739187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79180,
     "status": "ok",
     "timestamp": 1523881934596,
     "user": {
      "displayName": "Kareem Abdel Salam",
      "photoUrl": "//lh5.googleusercontent.com/-FFkZs03hTYw/AAAAAAAAAAI/AAAAAAAAJYE/suktak0dYDI/s50-c-k-no/photo.jpg",
      "userId": "111052780768832987513"
     },
     "user_tz": -120
    },
    "id": "CRRDBMtKiVQa",
    "outputId": "a3d86c3c-50f4-4b50-eb11-71f6b252bade"
   },
   "outputs": [],
   "source": [
    "# !rm -rf datalab\n",
    "# !wget -c http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# !ls\n",
    "# !uncompress aclImdb_v1.tar.gz\n",
    "# !tar -xvf aclImdb_v1.tar\n",
    "# !rm aclImdb_v1.tar\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exploratory Visualization ** is done by printing samples when reading files and file names to explore the dataset and its contents.\n",
    "\n",
    "Visualization is also done throughout the whole notebook by printing samples of results after each processing step to ensure correctness and for better readability.Also "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Benchmark: ** \n",
    "\n",
    "A good benchmark to see whether a machine-learning solution works or not is random chance.\n",
    "\n",
    "Given that the data is evenly split and the 25k test data is composed of 12.5 k positive samples and 12.5k negative samples then a random chance classifier will give a ** 50% ** accuracy,\n",
    "so my model must outperform this percentage in order to be effective.\n",
    "\n",
    "Another historical benchmark to compare my solution to is the paper mentioned before and in the Credits section buy Andrew Mass : the highest classification accuracy was ** 88.89% ** on this dataset.\n",
    "So an efficient model will give close numbers or outperform it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Data Preprocessing: ** removing HTML tags,removing stop-words and punctutations,and finally lemmatization.\n",
    "\n",
    "** Implementation: ** Feature extraction using TF-IDF, preparing training and testing data and then Supervised Learning using Logistic Regression and Naive Bayes.\n",
    "\n",
    "** Refinement: ** hyperparameter tuning of Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rhUtqFlm5WZ8"
   },
   "source": [
    "#### Reading files containing reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1166,
     "status": "ok",
     "timestamp": 1523881945504,
     "user": {
      "displayName": "Kareem Abdel Salam",
      "photoUrl": "//lh5.googleusercontent.com/-FFkZs03hTYw/AAAAAAAAAAI/AAAAAAAAJYE/suktak0dYDI/s50-c-k-no/photo.jpg",
      "userId": "111052780768832987513"
     },
     "user_tz": -120
    },
    "id": "ZKBSMyaZ0Ih8",
    "outputId": "ce6ea691-bd19-4c19-ac5b-edd77efdc017"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "function to read all text file names with their full paths in a given folder\n",
    "@param  path: given in this form '/path/' in linux\n",
    "'''\n",
    "def txt_file_names_list(path):\n",
    "    txt_files=glob.glob('.'+path+'*.txt')\n",
    "    return txt_files\n",
    "  \n",
    "#testing that function actually works\n",
    "text_files=txt_file_names_list('/aclImdb/train/pos/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** visualization checking correctness: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of text files in positive training set : 12500\n",
      "first 10 file names with paths:  ['./aclImdb/train/pos/9637_8.txt', './aclImdb/train/pos/9514_10.txt', './aclImdb/train/pos/5997_9.txt', './aclImdb/train/pos/10812_8.txt', './aclImdb/train/pos/4278_10.txt', './aclImdb/train/pos/4387_9.txt', './aclImdb/train/pos/8671_7.txt', './aclImdb/train/pos/4035_10.txt', './aclImdb/train/pos/7037_10.txt', './aclImdb/train/pos/3159_9.txt']\n"
     ]
    }
   ],
   "source": [
    "print(\"number of text files in positive training set :\",len(text_files))\n",
    "print(\"first 10 file names with paths: \",text_files[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fVfKVfb4R6nu"
   },
   "source": [
    "#### getting text files from folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UBsU7_Mq3TbY"
   },
   "outputs": [],
   "source": [
    "#made lists of names of files containing reviews (with their paths)\n",
    "pos_train_txt_names = txt_file_names_list('/aclImdb/train/pos/')\n",
    "neg_train_txt_names = txt_file_names_list('/aclImdb/train/neg/')\n",
    "pos_test_txt_names = txt_file_names_list('/aclImdb/test/pos/')\n",
    "neg_test_txt_names = txt_file_names_list('/aclImdb/test/neg/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cRmpm1FCSDJ6"
   },
   "source": [
    "#### reading the files to get the reviews' text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1670,
     "status": "ok",
     "timestamp": 1523881948630,
     "user": {
      "displayName": "Kareem Abdel Salam",
      "photoUrl": "//lh5.googleusercontent.com/-FFkZs03hTYw/AAAAAAAAAAI/AAAAAAAAJYE/suktak0dYDI/s50-c-k-no/photo.jpg",
      "userId": "111052780768832987513"
     },
     "user_tz": -120
    },
    "id": "jrosLxnP53sC",
    "outputId": "32e37c5d-95a6-418b-fff9-909f4687a149"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function capture_text_from_files()\n",
    "Given an array of strings of paths(names) of files to be read ,\n",
    "reads each file and saves all text in a string list\n",
    "where each element in the list is a full review\n",
    "\n",
    "@param text_file_names     list of text file names and paths to them\n",
    "\n",
    "@return text_files         list of contents of given text files\n",
    "'''\n",
    "def capture_text_from_files(text_file_paths):\n",
    "    text_files=[]\n",
    "    for file_path in text_file_paths:\n",
    "      f = open(file_path, 'r')\n",
    "      lines = f.readlines()\n",
    "      text=\"\"\n",
    "      f.close()\n",
    "      for line in lines:\n",
    "          text+=\" \"+line\n",
    "      text_files.append(text)\n",
    "    \n",
    "    return text_files\n",
    "      \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Exploratory visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of text review read : \n",
      " 12500\n",
      "\n",
      "name and path of the first review in the list:\n",
      "  ./aclImdb/train/pos/9637_8.txt\n",
      "\n",
      "content of the file contatining the first review:\n",
      "   This movie has a special way of telling the story, at first i found it rather odd as it jumped through time and I had no idea whats happening.<br /><br />Anyway the story line was although simple, but still very real and touching. You met someone the first time, you fell in love completely, but broke up at last and promoted a deadly agony. Who hasn't go through this? but we will never forget this kind of pain in our life. <br /><br />I would say i am rather touched as two actor has shown great performance in showing the love between the characters. I just wish that the story could be a happy ending.\n"
     ]
    }
   ],
   "source": [
    "#testing  the function and data exploration\n",
    "pos_reviews_train=capture_text_from_files(pos_train_txt_names )\n",
    "print(\"number of text review read : \\n\",len(pos_reviews_train))\n",
    "print(\"\\nname and path of the first review in the list:\\n \",pos_train_txt_names[0])\n",
    "print(\"\\ncontent of the file contatining the first review:\\n \",pos_reviews_train[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can check that the function correcly works by comparing printed review to the one in the file with the same name in the dataset files manually "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### final stage of reading reviews before starting preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "llQC_RRtkY5w"
   },
   "outputs": [],
   "source": [
    "#getting reviews text into lists to preprocess :\n",
    "pos_reviews_train_list=capture_text_from_files(pos_train_txt_names)\n",
    "neg_reviews_train_list=capture_text_from_files(neg_train_txt_names )\n",
    "pos_reviews_test_list=capture_text_from_files(pos_test_txt_names)\n",
    "neg_reviews_test_list=capture_text_from_files(neg_test_txt_names )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Samples **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_review_train=pos_reviews_train_list[0]\n",
    "neg_review_train=neg_reviews_train_list[0]\n",
    "pos_review_test=pos_reviews_test_list[0]\n",
    "neg_review_test=neg_reviews_test_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlOY9_015jgq"
   },
   "source": [
    "### Text Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JFaXMKB9EYxk"
   },
   "source": [
    "#### Removing HTML tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be clearly noticed from dataset exploration that the reviews containt HTML tags which will not be useful in the classification and may confuse the model and be interpreted as features.\n",
    "so the first preprocessing step is removing those tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8iq6RopT8Ope"
   },
   "outputs": [],
   "source": [
    "#removing html tags from reviews not to confuse classifiers\n",
    "'''\n",
    "Function to clear all html tags from all text in a list of strings\n",
    "\n",
    "@param text_list : list of strings targeted to clean\n",
    "'''\n",
    "def clean_html_from_list(text_list):\n",
    "\n",
    "  for i in range(len(text_list)):\n",
    "    text_list[i]=BeautifulSoup(text_list[i]).get_text()\n",
    "    \n",
    "  return text_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 17421
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 87984,
     "status": "ok",
     "timestamp": 1523882041892,
     "user": {
      "displayName": "Kareem Abdel Salam",
      "photoUrl": "//lh5.googleusercontent.com/-FFkZs03hTYw/AAAAAAAAAAI/AAAAAAAAJYE/suktak0dYDI/s50-c-k-no/photo.jpg",
      "userId": "111052780768832987513"
     },
     "user_tz": -120
    },
    "id": "enhesunHDn5I",
    "outputId": "b2960183-97c7-48e9-850f-14fce22699ab",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#using the function on data\n",
    "pos_reviews_train_list=clean_html_from_list(pos_reviews_train_list)\n",
    "neg_reviews_train_list=clean_html_from_list(neg_reviews_train_list)\n",
    "pos_reviews_test_list=clean_html_from_list(pos_reviews_test_list)\n",
    "neg_reviews_test_list=clean_html_from_list(neg_reviews_test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** visualization checking correctness: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie has a special way of telling the story, at first i found it rather odd as it jumped through time and I had no idea whats happening.Anyway the story line was although simple, but still very real and touching. You met someone the first time, you fell in love completely, but broke up at last and promoted a deadly agony. Who hasn't go through this? but we will never forget this kind of pain in our life. I would say i am rather touched as two actor has shown great performance in showing the love between the characters. I just wish that the story could be a happy ending.\n"
     ]
    }
   ],
   "source": [
    "print(pos_reviews_train_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see after printing the same text as before that the html tags were removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h5MmJzYLSdPw"
   },
   "source": [
    "#### Tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of splitting the long text into individual word list, this process helps in many things including stop words removal, lemmatizaion, determining the vocabulary size and makes extracting features easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "e8hSvGe41uNg"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize_list(list_words):\n",
    "  tokens=[]\n",
    "  for review in tqdm(list_words):\n",
    "    tokens.append(word_tokenize(review))\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 86584,
     "status": "ok",
     "timestamp": 1523882129498,
     "user": {
      "displayName": "Kareem Abdel Salam",
      "photoUrl": "//lh5.googleusercontent.com/-FFkZs03hTYw/AAAAAAAAAAI/AAAAAAAAJYE/suktak0dYDI/s50-c-k-no/photo.jpg",
      "userId": "111052780768832987513"
     },
     "user_tz": -120
    },
    "id": "M1pJtrWU2Ah8",
    "outputId": "03442e73-f1a8-47a3-f06d-ead1462b924f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:21<00:00, 571.98it/s]\n",
      "100%|██████████| 12500/12500 [00:23<00:00, 525.14it/s]\n",
      "100%|██████████| 12500/12500 [00:21<00:00, 574.49it/s]\n",
      "100%|██████████| 12500/12500 [00:22<00:00, 566.56it/s]\n"
     ]
    }
   ],
   "source": [
    "pos_reviews_train_list=tokenize_list(pos_reviews_train_list)\n",
    "neg_reviews_train_list=tokenize_list(neg_reviews_train_list)\n",
    "pos_reviews_test_list=tokenize_list(pos_reviews_test_list)\n",
    "neg_reviews_test_list=tokenize_list(neg_reviews_test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** visualization checking correctness: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tq5q4fFibk8C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'movie', 'has', 'a', 'special', 'way', 'of', 'telling', 'the', 'story', ',', 'at', 'first', 'i', 'found', 'it', 'rather', 'odd', 'as', 'it', 'jumped', 'through', 'time', 'and', 'I', 'had', 'no', 'idea', 'whats', 'happening.Anyway', 'the', 'story', 'line', 'was', 'although', 'simple', ',', 'but', 'still', 'very', 'real', 'and', 'touching', '.', 'You', 'met', 'someone', 'the', 'first', 'time', ',', 'you', 'fell', 'in', 'love', 'completely', ',', 'but', 'broke', 'up', 'at', 'last', 'and', 'promoted', 'a', 'deadly', 'agony', '.', 'Who', 'has', \"n't\", 'go', 'through', 'this', '?', 'but', 'we', 'will', 'never', 'forget', 'this', 'kind', 'of', 'pain', 'in', 'our', 'life', '.', 'I', 'would', 'say', 'i', 'am', 'rather', 'touched', 'as', 'two', 'actor', 'has', 'shown', 'great', 'performance', 'in', 'showing', 'the', 'love', 'between', 'the', 'characters', '.', 'I', 'just', 'wish', 'that', 'the', 'story', 'could', 'be', 'a', 'happy', 'ending', '.']\n"
     ]
    }
   ],
   "source": [
    "#print first review after tokenization\n",
    "print(pos_reviews_train_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis step : Word count and Dataset statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before further preprocessing it is good to find more about the dataset through some statistical analysis of the training set. (Analysing test set in theory is not necessary because it should be invisible to us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jNFhRlSdblAn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max\n",
      "2730\n",
      "min\n",
      "11\n",
      "mean\n",
      "264.498\n",
      "median\n",
      "198.0\n",
      "std\n",
      "196.29962566444183\n"
     ]
    }
   ],
   "source": [
    "lengths=[]\n",
    "for token_list in pos_reviews_train_list:\n",
    "    lengths.append(len(token_list))\n",
    "for token_list in neg_reviews_train_list:\n",
    "    lengths.append(len(token_list))\n",
    "list_of_lengths=np.asarray(lengths)\n",
    "maximum=np.max(list_of_lengths)\n",
    "minimum=np.min(list_of_lengths)\n",
    "mean=np.mean(list_of_lengths)\n",
    "median=np.median(list_of_lengths)\n",
    "std=np.std(list_of_lengths)\n",
    "print('max')\n",
    "print(maximum)\n",
    "print(\"min\")\n",
    "print(minimum)\n",
    "print('mean')\n",
    "print(mean)\n",
    "print('median')\n",
    "print(median)\n",
    "print('std')\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** comments on statistics:**\n",
    "as we can see from the cell above the maximum number of words in a review is 2730 while minimum number is 11 which is a big gap, but mean is 264.5 which means that this maximum is more of an outlier and this is show also by the median value and standard deviation. \n",
    "So usually we are dealing with reviews close to mean and median values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** visualization ** \n",
    "plotting a histogram of word count which is a plot showing how many reviews of the 25k training samples contain certain word counts.\n",
    "i.e. how many reviews contain less than 250 words and how many review contain less than 500 words ....etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"number of words per review\")\n",
    "plt.ylabel(\"number of reviews\")\n",
    "plt.hist(list_of_lengths, bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above we can see that the data is right skewed and most reviews contain less than 500 words and only small portion of data contain more than that and it keeps decreasing.\n",
    "So this may indicate that the reviews with word count > a certain value are outliers and may confuse classifiers while they may be special cases (i.e. features obtained from them cannot be generalized to fit the whole distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Samples **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "example of a positive sentiment training review:\n",
      "\n",
      "   This movie has a special way of telling the story, at first i found it rather odd as it jumped through time and I had no idea whats happening.<br /><br />Anyway the story line was although simple, but still very real and touching. You met someone the first time, you fell in love completely, but broke up at last and promoted a deadly agony. Who hasn't go through this? but we will never forget this kind of pain in our life. <br /><br />I would say i am rather touched as two actor has shown great performance in showing the love between the characters. I just wish that the story could be a happy ending.\n",
      "\n",
      "example of a negative sentiment training review:\n",
      "\n",
      "   I found this movie to be a great idea, that didn't deliver. It seems they found a way to build suspense, but couldn't stage their payoffs very well. In one case the police, are on the clock to find the hideout of the kidnappers. They painstakingly go from dentist to dentist to match a dental record. At the same time, the kidnapped man (Mason) escapes through the elevator shaft. After all the build up, the police arrive at the same time he gets free, which is very anti-climatic to say the least. There are also large narration scenes that take us \"inside the thinking\" of the terrorized husband and wife, which detracts from the suspense rather than adds to it. We are fully aware of their tension, and the voice-over is an insult and robs the viewer of any chance of a personal experience with the fear, as Hitchcock proved time and again, is far more effective. The greatest disappointment, is to sit through the whole movie, and the get the quick, rather bland ending. I mean it just...\"ends\" in a snore.\n",
      "\n",
      "example of a positive sentiment testing review:\n",
      "\n",
      "   If your expecting Jackass look somewhere else this an actual movie and for the budget well done the acting isnt top noth neither is the writing but the directing was there and so was the story definetly worth the rent and possibly the buy if you really enjoy it like i did. But for the person who just likes jackass rent it first.\n",
      "\n",
      "example of a negative sentiment testing review:\n",
      "\n",
      "   This show is just another bad comedy which will probably be cancelled after two seasons. It's not just that the jokes are sexist/racist/homophobic, they're also not funny and clichéd. In the first episode the Father said something along the lines of ' I wish women didn't go out and get jobs and have the same rights as men blah blah blah' That really helps attitudes huh? Then he was making fun of his son saying he was weird. What parent says their kid is weird? So overall this show is boring, unoriginal, offencive, clichéd and most of all NOT FUNNY. Yeah American Dad's offencive. But it does also make you laugh and is obviously taking the micky. Thats the difference.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nexample of a positive sentiment training review:\\n\\n \",pos_review_train)\n",
    "print(\"\\nexample of a negative sentiment training review:\\n\\n \",neg_review_train)\n",
    "print(\"\\nexample of a positive sentiment testing review:\\n\\n \",pos_review_test)\n",
    "print(\"\\nexample of a negative sentiment testing review:\\n\\n \",neg_review_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fMCqWhPWMWsM"
   },
   "source": [
    "#### Punctuation and Stop-words removal (is,are,the,this ....etc) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5YSOYKj0DGQm"
   },
   "source": [
    "removing any stop words written in capital or small letters and removing punctuations because all of those will just confuse the classifier and are present in all text disregarding its sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 832,
     "status": "ok",
     "timestamp": 1523882132748,
     "user": {
      "displayName": "Kareem Abdel Salam",
      "photoUrl": "//lh5.googleusercontent.com/-FFkZs03hTYw/AAAAAAAAAAI/AAAAAAAAJYE/suktak0dYDI/s50-c-k-no/photo.jpg",
      "userId": "111052780768832987513"
     },
     "user_tz": -120
    },
    "id": "xPy9rnkr1XD3",
    "outputId": "688931f8-e992-4f43-ab45-4d2bdcff6e01"
   },
   "outputs": [],
   "source": [
    "stop_words = list(get_stop_words('en')) \n",
    "\n",
    "def remove_stopwords(review):\n",
    "  filtered_words=[]\n",
    "  for tokens in review:\n",
    "    filtered_word=[]\n",
    "    filtered_word = [word for word in tokens if word.lower() not in stop_words]\n",
    "    filtered_words.append(filtered_word)\n",
    "    \n",
    "  return(filtered_words)\n",
    "def remove_punctuations(review):\n",
    "  filtered_words=[]\n",
    "  filtered=[]\n",
    "  for tokens in review:\n",
    "    filtered_word=[]\n",
    "    for token in tokens:\n",
    "        if (token.isalpha()== False):\n",
    "          filtered_word.append(token)\n",
    "    filtered_words=[x.lower() for x in tokens if x not in  filtered_word]\n",
    "    filtered.append(filtered_words)\n",
    "          \n",
    "    \n",
    "  return(filtered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 924,
     "status": "ok",
     "timestamp": 1523882134132,
     "user": {
      "displayName": "Kareem Abdel Salam",
      "photoUrl": "//lh5.googleusercontent.com/-FFkZs03hTYw/AAAAAAAAAAI/AAAAAAAAJYE/suktak0dYDI/s50-c-k-no/photo.jpg",
      "userId": "111052780768832987513"
     },
     "user_tz": -120
    },
    "id": "DT9oyL86bWGp",
    "outputId": "ca2ced46-b8f1-4f62-8561-5fbac0f5b721"
   },
   "outputs": [],
   "source": [
    "pos_train_tokens=remove_stopwords(pos_reviews_train_list)\n",
    "pos_train_tokens=remove_punctuations(pos_train_tokens)\n",
    "\n",
    "neg_train_tokens=remove_stopwords(neg_reviews_train_list)\n",
    "neg_train_tokens=remove_punctuations(neg_train_tokens)\n",
    "\n",
    "pos_test_tokens=remove_stopwords(pos_reviews_test_list)\n",
    "pos_test_tokens=remove_punctuations(pos_test_tokens)\n",
    "\n",
    "neg_test_tokens=remove_stopwords(neg_reviews_test_list)\n",
    "neg_test_tokens=remove_punctuations(neg_test_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**visualization and checking correctness:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie', 'special', 'way', 'telling', 'story', 'first', 'found', 'rather', 'odd', 'jumped', 'time', 'idea', 'whats', 'story', 'line', 'although', 'simple', 'still', 'real', 'touching', 'met', 'someone', 'first', 'time', 'fell', 'love', 'completely', 'broke', 'last', 'promoted', 'deadly', 'agony', 'go', 'will', 'never', 'forget', 'kind', 'pain', 'life', 'say', 'rather', 'touched', 'two', 'actor', 'shown', 'great', 'performance', 'showing', 'love', 'characters', 'just', 'wish', 'story', 'happy', 'ending']\n"
     ]
    }
   ],
   "source": [
    "print(pos_train_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as seen above all stop words and punctuations were removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJaDGMaSeMs0"
   },
   "source": [
    "**Lemmatization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJaDGMaSeMs0"
   },
   "source": [
    "Lemmatization is the process of getting the linguistic root of a word,it is better than stemming because for close words produces same output which helps in calculating term frequency in text.\n",
    "\n",
    "Lemmatization to be done efficiently needs the pos-tag of words which is its position in the sentence and usage (Noun,Verb,Adjective,Adverb).\n",
    "\n",
    "Also, inside lemmatization function some unhelpful words were removed because of the hypothesis that they are present in all reviews disregarding the sentiment like (movies,films,will,can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 900,
     "status": "ok",
     "timestamp": 1523882135238,
     "user": {
      "displayName": "Kareem Abdel Salam",
      "photoUrl": "//lh5.googleusercontent.com/-FFkZs03hTYw/AAAAAAAAAAI/AAAAAAAAJYE/suktak0dYDI/s50-c-k-no/photo.jpg",
      "userId": "111052780768832987513"
     },
     "user_tz": -120
    },
    "id": "-dsscCxY1eGm",
    "outputId": "02ed4b2a-12a4-494a-efb9-c2146f2a27f9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "      \n",
    "\n",
    "\n",
    "def lemmatize_list(list_of_lists_of_tokens):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  new_list=[]\n",
    "  for review in tqdm(list_of_lists_of_tokens):\n",
    "    new_review=[]\n",
    "    for token in review:\n",
    "      \n",
    "      if(token not in [\"movie\",\"film\",\"films\",\"movies\",\"will\",\"can\"]):\n",
    "        new_token=x=lemmatizer.lemmatize(token, get_wordnet_pos(nltk.pos_tag([token])[0][1]))\n",
    "        new_review.append(new_token)\n",
    "        \n",
    "    new_list.append(new_review)\n",
    "  return new_list\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [03:33<00:00, 58.52it/s]\n",
      "100%|██████████| 12500/12500 [03:38<00:00, 57.20it/s]\n",
      "100%|██████████| 12500/12500 [03:42<00:00, 56.14it/s]\n",
      "100%|██████████| 12500/12500 [03:24<00:00, 61.01it/s]\n"
     ]
    }
   ],
   "source": [
    "pos_train_tokens=lemmatize_list(pos_train_tokens)\n",
    "\n",
    "\n",
    "neg_train_tokens=lemmatize_list(neg_train_tokens)\n",
    "\n",
    "\n",
    "pos_test_tokens=lemmatize_list(pos_test_tokens)\n",
    "\n",
    "\n",
    "neg_test_tokens=lemmatize_list(neg_test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**visualization and checking correctness:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['special', 'way', 'tell', 'story', 'first', 'found', 'rather', 'odd', 'jumped', 'time', 'idea', 'whats', 'story', 'line', 'although', 'simple', 'still', 'real', 'touch', 'met', 'someone', 'first', 'time', 'fell', 'love', 'completely', 'broke', 'last', 'promote', 'deadly', 'agony', 'go', 'never', 'forget', 'kind', 'pain', 'life', 'say', 'rather', 'touch', 'two', 'actor', 'show', 'great', 'performance', 'show', 'love', 'character', 'just', 'wish', 'story', 'happy', 'end']\n"
     ]
    }
   ],
   "source": [
    "#printing text after lemmatization\n",
    "print(pos_train_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hQbIbT7JrqyW"
   },
   "source": [
    "as seen above the words returned to their roots and it has removed a word \"movie\" and other words that don't help in detecting sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QiM0-Xm0Dqtw"
   },
   "source": [
    "**Reconstructing reviews after preprocessing**\n",
    "\n",
    "After preprocessing has been done there is no need to keep the reviews tokenized and it is time to reconcatenate the words of each review together to have a final list of clean preprocessed reviews ready for feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6816,
     "status": "ok",
     "timestamp": 1523882142356,
     "user": {
      "displayName": "Kareem Abdel Salam",
      "photoUrl": "//lh5.googleusercontent.com/-FFkZs03hTYw/AAAAAAAAAAI/AAAAAAAAJYE/suktak0dYDI/s50-c-k-no/photo.jpg",
      "userId": "111052780768832987513"
     },
     "user_tz": -120
    },
    "id": "80axlXtZH_Uj",
    "outputId": "a5e48679-2e2d-47b4-aae0-f9bb806a8d7b"
   },
   "outputs": [],
   "source": [
    "def reconstruct_reviews(list_of_lists_of_tokens):\n",
    "    list_of_reviews=[]\n",
    "    for list_of_tokens in tqdm(list_of_lists_of_tokens):\n",
    "        review=\"\"\n",
    "        for token in list_of_tokens:\n",
    "            review+=token+\" \"\n",
    "        list_of_reviews.append(review)\n",
    "    return list_of_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:00<00:00, 51400.58it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 59121.94it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 60063.67it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 61373.35it/s]\n"
     ]
    }
   ],
   "source": [
    "pos_train_reviews=reconstruct_reviews(pos_train_tokens)\n",
    "neg_train_reviews=reconstruct_reviews(neg_train_tokens)\n",
    "pos_test_reviews=reconstruct_reviews(pos_test_tokens)\n",
    "neg_test_reviews=reconstruct_reviews(neg_test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**visualization and checking correctness:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special way tell story first found rather odd jumped time idea whats story line although simple still real touch met someone first time fell love completely broke last promote deadly agony go never forget kind pain life say rather touch two actor show great performance show love character just wish story happy end \n"
     ]
    }
   ],
   "source": [
    "#printing first review after reconstruction\n",
    "print(pos_train_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Training and testing data for the feature extractor and the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gkL9dEHxGwno"
   },
   "outputs": [],
   "source": [
    "#train data is list of positive and negative reviews full after preprocessing and aldo test data\n",
    "train_data=[]\n",
    "test_data=[]\n",
    "train_data.extend(pos_train_reviews)\n",
    "train_data.extend(neg_train_reviews)\n",
    "test_data.extend(pos_test_reviews)\n",
    "test_data.extend(neg_test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**visualization and checking correctness:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special way tell story first found rather odd jumped time idea whats story line although simple still real touch met someone first time fell love completely broke last promote deadly agony go never forget kind pain life say rather touch two actor show great performance show love character just wish story happy end \n",
      "\n",
      "surely one strangest theme history right ed wood impassioned defense glen glenda subject play bridge park avenue set play bohemian play russian speak questionable russian unconvincing accent speak english play restaurant one interested bridge one even despite great cast likely much interested bizarre young paul lukas fine well frank mchugh unlikely ghost writer lukas unlikely russian sunk fetishistic script \n"
     ]
    }
   ],
   "source": [
    "#making sure training data is well collected in one list\n",
    "print(train_data[0])\n",
    "print(\"\")\n",
    "print(train_data[12501])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Giving Labels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving positve sentiment reviews an integer label \"1\" and negative sentiment reviews a \"0\" to help the classifier learn and recongnize different classes and associate some text features to a label sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DDwGD3jchALx"
   },
   "outputs": [],
   "source": [
    "#making labels for reviews\n",
    "train_labels = np.asarray([1]*len(pos_train_reviews) + [0]*len(neg_train_reviews))\n",
    "test_labels = np.asarray([1]*len(pos_test_reviews) + [0]*len(neg_test_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**visualization and checking correctness:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "1 0\n",
      "(25000,)\n",
      "1 0\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape)\n",
    "print(train_labels[0],train_labels[12501])\n",
    "print(test_labels.shape)\n",
    "print(test_labels[0],test_labels[12501])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FTUPkCtQyv0"
   },
   "source": [
    "### Feature Extraction (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency - Inverse Document frequency) is a numerical feature extraction method for text data used to get more information about the text using Bag of words representation.\n",
    "ngram_range that is given as a parameter is the number of adjacent word associations made and I chose them to be pairs.\n",
    "\n",
    "TF-IDF method was chosen because Bag of Words methods according to the paper gave the highest results and other newer feature extraction methods like word2vec and doc2vec were not tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "O4RkCXoIXoTe"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import itertools\n",
    "obj = TfidfVectorizer(ngram_range=(1,2),stop_words='english')\n",
    "#fit transform to learn vocabulary from training data and then assign vectors to them\n",
    "train_vectors = obj.fit_transform(train_data)\n",
    "#transform the test data into vectors based on fitting done previously on training data\n",
    "test_vectors = obj.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**visualization and checking correctness:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train vector shape:\n",
      " (25000, 1482787)\n",
      "\n",
      "The first train vector:\n",
      "   (0, 1221909)\t0.059701768126163374\n",
      "  (0, 1430921)\t0.03877735519161602\n",
      "  (0, 1303621)\t0.053330787115042695\n",
      "  (0, 1249087)\t0.10584343964144066\n",
      "  (0, 907346)\t0.08063763893430748\n",
      "  (0, 693741)\t0.10830005917765659\n",
      "  (0, 1328104)\t0.06304653100654413\n",
      "  (0, 633254)\t0.056472791322568494\n",
      "  (0, 1440755)\t0.110662987492458\n",
      "  (0, 760205)\t0.052803146803002965\n",
      "  (0, 1187729)\t0.07066004023015644\n",
      "  (0, 1051197)\t0.04801518725448591\n",
      "  (0, 1341992)\t0.1334170501880486\n",
      "  (0, 836690)\t0.09013350534097873\n",
      "  (0, 476732)\t0.08628948311228088\n",
      "  (0, 782356)\t0.08181301940421053\n",
      "  (0, 254127)\t0.06081075270377014\n",
      "  (0, 156254)\t0.10090922976723118\n",
      "  (0, 1022405)\t0.0994605242151716\n",
      "  (0, 315886)\t0.09708322454624684\n",
      "  (0, 30450)\t0.11803716036369942\n",
      "  (0, 504328)\t0.0746511235412267\n",
      "  (0, 709456)\t0.05446723026214023\n",
      "  (0, 932065)\t0.08440488126645351\n",
      "  (0, 747523)\t0.04245031173716119\n",
      "  :\t:\n",
      "  (0, 1342390)\t0.16360856890119485\n",
      "  (0, 836888)\t0.15527440864053202\n",
      "  (0, 1329410)\t0.14978483761511796\n",
      "  (0, 476815)\t0.11175837050302972\n",
      "  (0, 782898)\t0.1522998180902537\n",
      "  (0, 254224)\t0.16360856890119485\n",
      "  (0, 156353)\t0.17022376405559536\n",
      "  (0, 1022435)\t0.17022376405559536\n",
      "  (0, 315892)\t0.17022376405559536\n",
      "  (0, 30466)\t0.17022376405559536\n",
      "  (0, 504566)\t0.17022376405559536\n",
      "  (0, 710390)\t0.17022376405559536\n",
      "  (0, 932217)\t0.15527440864053202\n",
      "  (0, 749680)\t0.12837641339222847\n",
      "  (0, 1132472)\t0.1522998180902537\n",
      "  (0, 1342000)\t0.1589150132446542\n",
      "  (0, 14297)\t0.10706481484648912\n",
      "  (0, 558331)\t0.09567131018929531\n",
      "  (0, 955104)\t0.14241066474387654\n",
      "  (0, 782795)\t0.11269091452397265\n",
      "  (0, 202648)\t0.09822861255253038\n",
      "  (0, 698241)\t0.11269091452397265\n",
      "  (0, 1450943)\t0.14978483761511796\n",
      "  (0, 1250505)\t0.14978483761511796\n",
      "  (0, 580615)\t0.10090922976723118\n",
      "\n",
      "test vector shape:\n",
      " (25000, 1482787)\n",
      "\n",
      "The first test vector:\n",
      "   (0, 1468584)\t0.13778521664168836\n",
      "  (0, 1468232)\t0.07100684300440592\n",
      "  (0, 1466139)\t0.1792229228087842\n",
      "  (0, 1465700)\t0.08116479389994126\n",
      "  (0, 1249087)\t0.05016744148187213\n",
      "  (0, 1079118)\t0.1930891194462571\n",
      "  (0, 1055932)\t0.13227397501424926\n",
      "  (0, 1055213)\t0.05082352001502214\n",
      "  (0, 996852)\t0.2420468595223042\n",
      "  (0, 996805)\t0.10747884246152573\n",
      "  (0, 958239)\t0.20047991092777\n",
      "  (0, 957785)\t0.09038563581933172\n",
      "  (0, 897443)\t0.22078987140331835\n",
      "  (0, 774195)\t0.2420468595223042\n",
      "  (0, 774173)\t0.05374060909046003\n",
      "  (0, 756320)\t0.18862928989937958\n",
      "  (0, 752286)\t0.07969128229313469\n",
      "  (0, 696464)\t0.11475721755310588\n",
      "  (0, 694580)\t0.043353731236151424\n",
      "  (0, 678292)\t0.36333435349315885\n",
      "  (0, 675943)\t0.19690377613164547\n",
      "  (0, 440613)\t0.08046426897115912\n",
      "  (0, 413632)\t0.17772569651442652\n",
      "  (0, 412962)\t0.07495264132977257\n",
      "  (0, 351225)\t0.20047991092777\n",
      "  (0, 350291)\t0.08405866634850227\n",
      "  (0, 324683)\t0.2420468595223042\n",
      "  (0, 324676)\t0.20715383458914413\n",
      "  (0, 168696)\t0.20249848401002968\n",
      "  (0, 168401)\t0.10467242394927342\n",
      "  (0, 161170)\t0.20988627801836535\n",
      "  (0, 161163)\t0.09292152991260436\n",
      "  (0, 16570)\t0.22596656877033477\n",
      "  (0, 16527)\t0.10595092681372387\n",
      "  (0, 10471)\t0.2420468595223042\n",
      "  (0, 9388)\t0.054226816837046374\n"
     ]
    }
   ],
   "source": [
    "print(\"train vector shape:\\n\",train_vectors.shape)\n",
    "print(\"\\nThe first train vector:\\n\",train_vectors[0])\n",
    "print(\"\\ntest vector shape:\\n\",test_vectors.shape)\n",
    "print(\"\\nThe first test vector:\\n\",test_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Logistic regression and Naive Bayes supervised learning classifiers, fitting them on input training vectors then outputing the classification accuracy as a percentage.\n",
    "\n",
    "I chose Naive Bayes because it is famous for its good results with text data and Logistic Regression because it was the only classifier allowed in the 2016 kaggle challenge and it produced good results in binary classification.\n",
    "\n",
    "The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12736,
     "status": "ok",
     "timestamp": 1523882193566,
     "user": {
      "displayName": "Kareem Abdel Salam",
      "photoUrl": "//lh5.googleusercontent.com/-FFkZs03hTYw/AAAAAAAAAAI/AAAAAAAAJYE/suktak0dYDI/s50-c-k-no/photo.jpg",
      "userId": "111052780768832987513"
     },
     "user_tz": -120
    },
    "id": "5lXFss-zIR6R",
    "outputId": "1f104235-d91a-4c7c-82d4-97239486dbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression classification accuracy:\n",
      " 86.868 %\n"
     ]
    }
   ],
   "source": [
    "##logistic Regression\n",
    "from sklearn import linear_model, datasets\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(train_vectors, train_labels)\n",
    "print(\"Logistic Regression classification accuracy:\\n\",logreg.score(test_vectors,test_labels)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 980,
     "status": "ok",
     "timestamp": 1523882257680,
     "user": {
      "displayName": "Kareem Abdel Salam",
      "photoUrl": "//lh5.googleusercontent.com/-FFkZs03hTYw/AAAAAAAAAAI/AAAAAAAAJYE/suktak0dYDI/s50-c-k-no/photo.jpg",
      "userId": "111052780768832987513"
     },
     "user_tz": -120
    },
    "id": "2TjzAgKLo5jE",
    "outputId": "1cd373bd-ce38-41f1-e5be-026c0c87be7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes classification accuracy:\n",
      " 84.676 %\n"
     ]
    }
   ],
   "source": [
    "### Naive-bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_vectors, train_labels)\n",
    "print(\"Naive Bayes classification accuracy:\\n\",nb.score(test_vectors,test_labels)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving results using GridsearchCV where I tune the 'C' hyper parameter in Logistic regression to get better results.\n",
    "The tuning is done on 10% of the training data : 2500 samples , 1250 positive and 1250 negative to get the best hyperparmeter values and extract the best estimator, then this estimator with best parameters is trained on all training data and accuracy is produced.\n",
    "\n",
    "Gridsearch is a hyperparameter optimization algorithm that tries all input hyperparameter values given on some portion of data to find best hyperparameters and best model to operate on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting 10% of training data and assigning their labels\n",
    "train_vectors10=train_vectors[11250:13750]\n",
    "train_labels10=[1] * 1250\n",
    "train_labels10.extend([0]*1250)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying different values for C hyperparameter ([0.1,1,10,25,50,100,1000,10000]) to find the best of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kareem/.local/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/kareem/.local/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import grid_search\n",
    "#values of C hyper parameter to try in grid search\n",
    "parameters = {'C':[0.1,1,10,25,50,100,1000,10000]}\n",
    "#making a new logistic regression model\n",
    "logreg = linear_model.LogisticRegression()\n",
    "grid_search = grid_search.GridSearchCV(logreg, parameters)\n",
    "#fitting grid search with 10% of training data to find best parameters\n",
    "grid_search.fit(train_vectors10, train_labels10)\n",
    "# extracting estimator with best parameters\n",
    "logclf=grid_search.best_estimator_\n",
    "logclf_CV=grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for accuracy:\n",
      "{'C': 50}\n",
      "\n",
      "estimator:\n",
      "\n",
      "LogisticRegression(C=50, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "Logistic Regression classification accuracy:\n",
      " 87.928 %\n"
     ]
    }
   ],
   "source": [
    "print('Best params for accuracy:')\n",
    "print(grid_search.best_params_)\n",
    "print(\"\\nestimator:\\n\")\n",
    "print(logclf)\n",
    "logclf.fit(train_vectors,train_labels)\n",
    "print(\"\\nLogistic Regression classification accuracy:\\n\",logclf.score(test_vectors,test_labels)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as seen above, the best hyperparameter value was C=50 and the best estimator with this parameter value gave  87.928 % accuracy compared to  86.868 % before tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Tuning Naive Bayes**\n",
    "\n",
    "Trying different alpha (smoothening) values: [0,0.3,0.5,0.7,1] using gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kareem/.local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/kareem/.local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/kareem/.local/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "#values of C hyper parameter to try in grid search\n",
    "parameters = {'alpha':[0,0.3,0.5,0.7,1]}\n",
    "#making a new naive bayes model\n",
    "nb = MultinomialNB()\n",
    "grid_search = GridSearchCV(nb, parameters)\n",
    "#fitting grid search with 10% of training data to find best parameters\n",
    "grid_search.fit(train_vectors10, train_labels10)\n",
    "# extracting estimator with best parameters\n",
    "clf=grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for accuracy:\n",
      "{'alpha': 0.5}\n",
      "\n",
      "estimator:\n",
      "\n",
      "MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True)\n",
      "\n",
      "Naive Bayes classification accuracy:\n",
      " 84.63199999999999 %\n"
     ]
    }
   ],
   "source": [
    "print('Best params for accuracy:')\n",
    "print(grid_search.best_params_)\n",
    "print(\"\\nestimator:\\n\")\n",
    "print(clf)\n",
    "clf.fit(train_vectors,train_labels)\n",
    "print(\"\\nNaive Bayes classification accuracy:\\n\",clf.score(test_vectors,test_labels)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as seen above, the best hyperparameter value found by grid search was alpha=0.5 and the best estimator with this parameter value gave 84.63199999999999% accuracy compared to 84.676 % before tuning, so this shows that tuning on 10% failed to extract best alpha parameter and that the higher smoothening value gave higher accuracy (by a small value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Folds Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides hyperparameter tuning using Grid Search, K-Folds Cross Validation is now used to validate the robustness of the model by making several train-validation splits withing data and train the model on them and test on different validation sets, to detect overfitting and underfitting signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(logclf_CV, train_vectors, train_labels, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[88.78 88.24 88.9  89.44 89.24]\n",
      "Mean Accuracy: 88.92  (+/- 0.01 deviation) \n"
     ]
    }
   ],
   "source": [
    "print(scores*100)\n",
    "print(\"Mean Accuracy: %0.2f  (+/- %0.2f deviation) \" % (scores.mean()*100, scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above when used K-Folds Cross Validation on the training set with our best model that came out of gridsearchCV hyperparameter tuning, high accuracies were obtained on training set even higher than ones obtained before on testing test but not much higher which shows very good generalization and low tendency towards overfitting and underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another method for K-folds cross validation\n",
    "# from sklearn.model_selection import KFold\n",
    "# mean=0\n",
    "# kf = KFold(n_splits=5,shuffle=True,random_state=1995)\n",
    "# for train, test in kf.split(train_vectors):\n",
    "#     print('train: %s, test: %s' % (train_vectors[train].shape, train_vectors[test].shape))\n",
    "#     logclf_CV.fit(train_vectors[train],train_labels[train])\n",
    "#     x=logclf_CV.score(train_vectors[test],train_labels[test])\n",
    "#     mean+=x\n",
    "#     print(\"accuracy\",x*100,\"%\")\n",
    "# print(\"mean K-Folds cross validation score:\",mean/5*100,\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and techniques \n",
    "after each step there is a breif description of each Algorithm and Technique used followed by reason of usage.\n",
    "\n",
    "** Naive Bayes ** is a probabilistic classifier that uses the probability of appearance of features in certain classes to predict the correct class of a given input but it makes an assumption of having independence between features.It is a conditional probability model given a problem instance to be classified, represented by a vector X=(x1,x2...xn) representing some n features (independent variables), it assigns to this instance probabilities P(C_k/ x1,x2...xn) for each of K possible outcomes or classes C_k.\n",
    "\n",
    "We Classify and give a label to the review according to the class that has the highest conditional probability given the features as conditions.\n",
    "\n",
    "\n",
    "** Logistic Regression ** is an algorithm mostly used for binary classification by fitting a the best curve possible through tuning linear equation coefficients to make a decision boundary between classes through searching for minimal mean squared loss.\n",
    "\n",
    "Logistic regression uses an equation as the representation, input values (x) are combined linearly using weights or coefficient values (b1):\n",
    "\n",
    "y = e^(b0 + b1*x) / (1 + e^(b0 + b1*x)).\n",
    "\n",
    "Where y is the predicted output, b0 is the bias or intercept term and b1 is the coefficient for the single input value (x). Each column in your input data has an associated b coefficient (a constant real value) that must be learned from your training data.\n",
    "\n",
    "(resource: https://machinelearningmastery.com/logistic-regression-for-machine-learning/ )\n",
    "\n",
    "A good thing about Logistic Regression beside the fact that it runs in little time in this task is the fact that it supports online learning (there are implementations of it that support it), this means that we can add more training examples if they become available in the future without needing to rerun the algorithm on the whole dataset again.\n",
    "(resource: “A Family of Online Boosting Algorithms” by Babenco et al.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuned C parameter of logistic regression is the inverse of regulization strength (higher values of C correspond to less regularization) which means on increasing the value of C we allow the equation to get to higher degrees in order to fit the problem better which results in more complexity.\n",
    "So by tuning the best value for C found that gives best accuracy was 50 as compare to the default value 1 which means we need to decrease regularizarion and allow more degrees of freedom to make better classification.\n",
    "But in comparing results we find that the accuracy was not improved by much (about 1.2%) so this improvement can be removed if we wanted less complexity.\n",
    "\n",
    "The alpha parameter for naive bayes is additive (Laplace/Lidstone) smoothing parameter: 0 for no smoothing and 1 for full smoothing.\n",
    "Smoothening aims to reduce the effect of rare words in classification, for example: if a word \"hot\" is found only in one negative sentiment review , we want to reduce the effect of it in classification of all reviews containing this word as negative just because of it.\n",
    "The accuracy values show that the default smoothening value (alpha=1) gave better results by a little amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best accuracy obtained was that of logistic regression equivalent to  87.928 %, which is much better than the random chance benchmark (50%) and very close to the other stated benchmark by Maas et al. ( 88.89%) only off by 1%.\n",
    "So this solution succeeds to solve the problem to a good extent and can classify most text sentiments of movie reviews especially that it had been trained on 25k different reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free-Form Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10906  1594]\n",
      " [ 2248 10252]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "test_predicted=clf.predict(test_vectors)\n",
    "cm=confusion_matrix(test_labels, test_predicted)\n",
    "plot_confusion_matrix(cm, labels=[\"positive\",\"negative\"])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7efd51ea18d0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#trying seaborn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.heatmap(confusion_matrix(test_labels, test_predicted), annot = True, fmt = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above by plotting the confusion matrix  we see that predicting positive sentiment is the best performance of the model is in predicting True-positive labels and there is some sort of diagonal symmetry within the matrix, so the model is balanced and does not tend towards one type of misclassification over others by a big value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project summary is that in order to classify the sentiment of reviews I needed to preprocess the text first to remove any words that don't help in classification and then extract numerical features based on word frequency in documents to give weights to words and finally use supervised learning to process those features and make decision boundaries to classify any test document given its numerical features.\n",
    "\n",
    "What I found challenging about the project was understanding word embeddings and extraction of features from text but it was a thing worth learning.\n",
    "Another challenging thing was all the documentation required and explanations of each step which is so time consuming but it is necessary to produce any readable output that is valuable to others.\n",
    "\n",
    "In the process of coding the only thing I struggled with was in the tuning step where I used train_test_split() function of scikit learn to get 10% of data to use for tuning but the tuning wouldn't improve accuracy based on those splits and hyperparameters did not change but when I hand split the 10% I wanted the tuning worked and accuracy improved and this is probably because I chose some baised unshuffled data but they seemed to work for tuning, so it was not fully automated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think what can be improved is trying another word embedding models like word2vec and doc2vec to extract features in other ways and maybe use other classification methods like deep learning ones (CNNs or RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZRhvaZZlS1VE"
   },
   "source": [
    "**Dataset credits and copyrights:**\n",
    "\n",
    "InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  \n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  \n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  \n",
    "  month     = {June},\n",
    "  \n",
    "  year      = {2011},\n",
    "  \n",
    "  address   = {Portland, Oregon, USA},\n",
    "  \n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  \n",
    "  pages     = {142--150},\n",
    "  \n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n",
    "\n",
    "\n",
    "References\n",
    "\n",
    "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
    "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
    "636-659.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Assignment#2-SentimentAnalysis.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
